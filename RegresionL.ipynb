{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec506b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Semana 1: Ejemplo de regresión lineal ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datos ficticios: m2 de casas y precios\n",
    "metros = np.array([50, 60, 80, 100, 120, 150, 200]).reshape(-1, 1)\n",
    "precios = np.array([100, 120, 160, 200, 240, 300, 400])\n",
    "\n",
    "# Separar en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(metros, precios, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear modelo\n",
    "modelo = LinearRegression()\n",
    "\n",
    "# Entrenar\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "predicciones = modelo.predict(X_test)\n",
    "\n",
    "# Resultados\n",
    "print(\"Coeficiente (pendiente):\", modelo.coef_[0])\n",
    "print(\"Intercepto:\", modelo.intercept_)\n",
    "print(\"Predicciones:\", predicciones)\n",
    "print(\"Valores reales:\", y_test)\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(metros, precios, color=\"blue\", label=\"Datos reales\")\n",
    "plt.plot(metros, modelo.predict(metros), color=\"red\", label=\"Modelo\")\n",
    "plt.xlabel(\"Metros cuadrados\")\n",
    "plt.ylabel(\"Precio (miles)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c202836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Comparador de modelos clásicos + MLP en 3 datasets\n",
    "# Datasets: \"moons\", \"circles\", \"classification\"\n",
    "# Modelos: Regresión Logística Machine Learning, KNN Machine Learning, SVM-RBF Machine Learning, MLP DEEP LEARNING\n",
    "# Métricas: Accuracy, Precision, Recall, F1, AUC + CV 5-fold\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    ")\n",
    "\n",
    "# ---------- CONFIGURACIÓN ----------\n",
    "RANDOM_STATE = 42\n",
    "DATASET_CHOICE = \"circles\"         # <-- \"moons\" | \"circles\" | \"classification\"\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "# Dificultad de los datasets 2D\n",
    "MOONS_NOISE = 0.35\n",
    "CIRCLES_NOISE = 0.25\n",
    "CIRCLES_FACTOR = 0.5  # separación entre círculos (0-1). Más bajo = más difícil\n",
    "\n",
    "# Tamaño y dificultad del dataset tabular\n",
    "CLASSIF_SAMPLES = 10000\n",
    "CLASSIF_FEATURES = 20\n",
    "CLASSIF_INFORMATIVE = 8\n",
    "CLASSIF_REDUNDANT = 2\n",
    "CLASSIF_CLUSTERS_PER_CLASS = 2\n",
    "CLASSIF_CLASS_SEP = 1.2\n",
    "CLASSIF_FLIP_Y = 0.02  # ruido de etiquetas\n",
    "\n",
    "# ---------- CREACIÓN DEL DATASET ----------\n",
    "if DATASET_CHOICE == \"moons\":\n",
    "    X, y = make_moons(n_samples=10000, noise=MOONS_NOISE, random_state=RANDOM_STATE)\n",
    "elif DATASET_CHOICE == \"circles\":\n",
    "    X, y = make_circles(n_samples=10000, noise=CIRCLES_NOISE, factor=CIRCLES_FACTOR, random_state=RANDOM_STATE)\n",
    "elif DATASET_CHOICE == \"classification\":\n",
    "    X, y = make_classification(\n",
    "        n_samples=CLASSIF_SAMPLES,\n",
    "        n_features=CLASSIF_FEATURES,\n",
    "        n_informative=CLASSIF_INFORMATIVE,\n",
    "        n_redundant=CLASSIF_REDUNDANT,\n",
    "        n_clusters_per_class=CLASSIF_CLUSTERS_PER_CLASS,\n",
    "        class_sep=CLASSIF_CLASS_SEP,\n",
    "        flip_y=CLASSIF_FLIP_Y,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"DATASET_CHOICE debe ser 'moons', 'circles' o 'classification'.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ---------- MODELOS ----------\n",
    "modelos = {\n",
    "    \"Logística\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(C=1.0, max_iter=1000, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"KNN\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier(n_neighbors=15, weights=\"distance\"))\n",
    "    ]),\n",
    "    \"SVM-RBF\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"MLP\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", MLPClassifier(hidden_layer_sizes=(64, 64),\n",
    "                              activation=\"relu\",\n",
    "                              alpha=1e-4,\n",
    "                              learning_rate_init=1e-2,\n",
    "                              max_iter=400,\n",
    "                              random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ---------- EVALUACIÓN ----------\n",
    "def eval_model(nombre, modelo, X_train, y_train, X_test, y_test):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    # Probabilidades para AUC\n",
    "    if hasattr(modelo[-1], \"predict_proba\"):\n",
    "        y_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(modelo[-1], \"decision_function\"):\n",
    "        scores = modelo.decision_function(X_test)\n",
    "        # normalizar a [0,1] para AUC si no es proba\n",
    "        y_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "    else:\n",
    "        y_proba = None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "\n",
    "    print(f\"\\n=== {nombre} ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1       : {f1:.4f}\")\n",
    "    print(f\"AUC      : {auc:.4f}\")\n",
    "    print(\"Matriz de confusión:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Reporte de clasificación:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    # Curva ROC (opcional si hay probas)\n",
    "    if y_proba is not None and X.shape[1] > 0:\n",
    "        RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "        plt.title(f\"Curva ROC - {nombre} ({DATASET_CHOICE})\")\n",
    "        plt.show()\n",
    "\n",
    "    return acc, f1, auc\n",
    "\n",
    "resultados = {}\n",
    "for nombre, modelo in modelos.items():\n",
    "    resultados[nombre] = eval_model(nombre, modelo, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n--- Resumen (Accuracy, F1, AUC) ---\")\n",
    "for k, (acc, f1, auc) in resultados.items():\n",
    "    print(f\"{k:10s}  Acc={acc:.4f}  F1={f1:.4f}  AUC={auc:.4f}\")\n",
    "\n",
    "# ---------- VALIDACIÓN CRUZADA ----------\n",
    "print(\"\\n=== Validación cruzada 5-fold (Accuracy) ===\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "for nombre, modelo in modelos.items():\n",
    "    scores = cross_val_score(modelo, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    print(f\"{nombre:10s}  mean={scores.mean():.4f}  +/- {scores.std():.4f}\")\n",
    "\n",
    "# ---------- FRONTERA DE DECISIÓN (solo para 2D) ----------\n",
    "def plot_decision_boundary(modelo, X, y, titulo):\n",
    "    if X.shape[1] != 2:\n",
    "        print(\"Plot omitido: dataset no es 2D.\")\n",
    "        return\n",
    "    modelo.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n",
    "                         np.linspace(y_min, y_max, 400))\n",
    "    Z = modelo.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"k\", s=12)\n",
    "    plt.title(titulo)\n",
    "    plt.xlabel(\"Feature 1\"); plt.ylabel(\"Feature 2\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "if DATASET_CHOICE in (\"moons\", \"circles\"):\n",
    "    for nombre, modelo in modelos.items():\n",
    "        plot_decision_boundary(modelo, X, y, f\"Frontera - {nombre} ({DATASET_CHOICE})\")\n",
    "\n",
    "#notas: KNN: Se comporta bien en problemas simples o de baja dimensión.\n",
    "\n",
    "#En datasets grandes o con mucho ruido, puede volverse ineficiente y perder precisión.\n",
    "\n",
    "#SVM con kernel RBF:\n",
    "\n",
    "#Muy robusto en datasets medianos (lineales y no lineales).\n",
    "\n",
    "#Puede escalar mal a datasets muy grandes (tiempo de cómputo).\n",
    "\n",
    "#MLP (red neuronal):\n",
    "\n",
    "#Flexible tanto en lineal como en no lineal.\n",
    "\n",
    "#Pero requiere más datos, ajuste y regularización para no caer en overfitting.\n",
    "\n",
    "#Logística:\n",
    "\n",
    "#Sigue siendo muy útil como baseline y en problemas realmente lineales.\n",
    "\n",
    "#Ventaja: interpreta coeficientes, da probabilidades claras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96310468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
